# Project Answers - Optimized Tensor Operations Compiler

## 1. What problem does your project address, what is the research question or hypothesis, and why is the problem algorithmically relevant and non-trivial?

Our project addresses the problem of optimizing multidimensional tensor operations to reduce execution time. The research question is: can we create a compiler that analyzes tensor code, converts it into a graph, and automatically applies optimizations that reduce memory accesses and computational cost? This problem is algorithmically relevant because tensor operations are fundamental in machine learning and scientific computing, but writing optimized code manually is difficult and error-prone. When developers write tensor code manually, they often miss opportunities to reuse calculations, fuse operations together, or optimize memory access patterns. This leads to slower execution and wasted computational resources. It's non-trivial because it requires building a dependency graph to understand how operations relate to each other, identifying optimizable patterns like duplicate calculations or operations that can be combined, and generating efficient parallel code for GPUs that can execute thousands of threads simultaneously. The problem involves multiple computer science areas including graph analysis to understand dependencies, compiler optimization techniques to transform code, and parallel programming to generate efficient GPU kernels. Additionally, we integrate computational geometry to represent tensor iteration spaces as geometric regions, which allows for mathematical analysis of data access patterns and enables more sophisticated optimizations.

## 2. What are the primary objectives of your project, what baseline or reference point is used for comparison, and what measurable criteria determine whether the project succeeds?

The main objectives are to reduce the number of memory accesses and computational cost of tensor operations through automatic optimizations, and to generate efficient parallel CUDA code that can leverage GPU hardware effectively. We also aim to represent tensor iteration spaces using computational geometry, which helps analyze and optimize data access patterns. The baseline for comparison is unoptimized code where each operation runs independently without reusing calculations or fusing operations. In this baseline, if the same calculation appears twice, it gets computed twice. Operations that could run together are executed separately, leading to multiple memory transfers and kernel launches. The project is considered successful if it achieves significant metric reductions: specifically, we aimed for at least 50% reduction in graph operations, 40% reduction in memory accesses, and 80% reduction in redundant computational operations. In our experimental results, we achieved 50% reduction in operations (from 6 nodes down to 3), 61% reduction in memory accesses, and 99% reduction in computation, significantly exceeding our initial goals. These metrics demonstrate that the compiler successfully identifies and eliminates redundant work, fuses operations when possible, and generates more efficient code structure overall.

## 3. What is the core algorithmic approach used in your solution, which data structures were implemented, and why are these choices theoretically justified for achieving correctness and efficiency?

The core approach is to build a directed acyclic graph (DAG) where each node represents a tensor operation and edges represent data dependencies. When we parse the input code, we construct this graph incrementally, adding nodes for each operation and connecting them based on which tensors they use as inputs. We implemented several key data structures: graphs with dynamic arrays for storing nodes and edges, which allows the graph to grow efficiently as we process more operations. We use hash tables for the symbol table to quickly look up tensor information by name, providing O(1) average-case lookup time. We also created structures to represent tensor dimensions as arrays of integers, and geometric iteration spaces as hyper-rectangles with lower and upper bounds for each dimension. These choices are theoretically justified because graphs are the natural structure to represent dependencies between operations, allowing us to identify which operations can run in parallel and which must wait for previous results. The DAG structure ensures we can always determine a valid execution order. Hash tables provide O(1) lookup for variables, which is crucial for fast symbol resolution during parsing and code generation. Dynamic arrays allow efficient graph growth with amortized O(1) insertion cost, making the compiler scalable. Iteration spaces as hyper-rectangles enable formal mathematical analysis of dependencies through operations like intersection and union, which can help identify optimization opportunities and verify correctness of loop transformations.

## 4. What is the formal time and space complexity of your implementation, how does it scale as input size grows, and what factors dominate performance?

The dominant time complexity is O(n²) due to common subexpression elimination (CSE), where n is the number of operations in the program. In the CSE algorithm, for each new operation we check if it's a duplicate by comparing it against all previous operations, leading to n comparisons for each of n operations. Graph construction is O(n) amortized because we add nodes one at a time, and while we occasionally need to resize the array, the amortized cost per insertion is constant. Operation fusion is O(n) since we make a single pass through the graph. CUDA code generation is O(n) because we visit each node once to generate its corresponding kernel code. Space complexity is O(n + e) where e is the number of edges in the dependency graph. In the worst case, if each operation depends on all previous ones, we could have O(n²) edges, but in practice most operations depend on only a few previous results, so the space usage is typically closer to O(n). As input size grows, the dominant factor is duplicate search in CSE, which scales quadratically. However, this can be improved to O(n) using hash tables instead of linear search, where we hash the operation type and input identifiers to quickly find duplicates. For most practical cases with tens or hundreds of operations, the current performance is acceptable, but for very large programs with thousands of operations, CSE optimization becomes the bottleneck and the hash table improvement would be necessary. The other phases of compilation scale linearly and remain efficient even for large programs.

## 5. What experimental results were obtained, how do they support or contradict the theoretical expectations, and how was previously received feedback incorporated into the final system and results?

Experimental results show significant improvements: 50% reduction in number of operations (from 6 to 3 nodes), 61% reduction in memory accesses, and 99% reduction in redundant computational operations. These results support theoretical expectations that fusion and duplicate elimination optimizations can significantly reduce computational cost. The 99% computation reduction is even better than expected, probably because we eliminated duplicate calculations that weren't initially obvious - for example, when the same tensor appears multiple times in different operations, we were able to identify and reuse those calculations. The memory reduction of 61% shows that our optimizations successfully reduce the number of times data needs to be loaded from memory, which is crucial for GPU performance. Feedback received during development, especially about parsing issues and input format problems with different shells, was incorporated by improving error handling to provide clearer messages and by clearly documenting the correct usage format with printf. We also received feedback about the need for visualizations to understand the optimizations, which led us to create graphs showing before-and-after comparisons of operations, memory usage, and computational cost. Additionally, we identified and documented limitations like the O(n²) complexity of CSE and lack of complete shape verification, which provides a clear roadmap for future improvements. The feedback process helped us create a more robust and user-friendly system.

