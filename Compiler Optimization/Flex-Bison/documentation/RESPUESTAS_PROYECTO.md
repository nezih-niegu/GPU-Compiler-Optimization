# Respuestas sobre el Proyecto - Compilador Optimizado para Operaciones Tensoriales

## 1. What problem does your project address, what is the research question or hypothesis, and why is the problem algorithmically relevant and non-trivial?

El proyecto aborda el problema de optimizar operaciones con tensores multidimensionales para reducir el tiempo de ejecución. La pregunta de investigación es: ¿podemos crear un compilador que analice código tensorial, lo convierta en un grafo, y aplique optimizaciones automáticas que reduzcan accesos a memoria y costo computacional? El problema es relevante porque las operaciones tensoriales son fundamentales en machine learning y computación científica, pero escribir código optimizado manualmente es difícil y propenso a errores. Es no-trivial porque requiere construir un grafo de dependencias, identificar patrones optimizables, y generar código paralelo eficiente para GPUs, lo cual involucra múltiples áreas de ciencias de la computación como análisis de grafos, optimización de compiladores, y programación paralela.

## 2. What are the primary objectives of your project, what baseline or reference point is used for comparison, and what measurable criteria determine whether the project succeeds?

Los objetivos principales son reducir el número de accesos a memoria y el costo computacional de operaciones tensoriales mediante optimizaciones automáticas, y generar código CUDA paralelo eficiente. El punto de referencia es el código sin optimizar, donde cada operación se ejecuta de forma independiente sin reutilizar cálculos ni fusionar operaciones. El proyecto se considera exitoso si logra reducir significativamente las métricas: específicamente, buscamos al menos 50% de reducción en operaciones del grafo, 40% de reducción en accesos a memoria, y 80% de reducción en operaciones computacionales redundantes. En nuestros resultados, logramos 50% de reducción en operaciones, 61% de reducción en memoria, y 99% de reducción en computación, superando los objetivos iniciales.

## 3. What is the core algorithmic approach used in your solution, which data structures were implemented, and why are these choices theoretically justified for achieving correctness and efficiency?

El enfoque central es construir un grafo dirigido acíclico (DAG) donde cada nodo representa una operación tensorial y las aristas representan dependencias de datos. Implementamos estructuras de datos como grafos con arrays dinámicos para nodos y aristas, tablas hash para la tabla de símbolos, y estructuras para representar dimensiones de tensores y espacios de iteración geométricos. Estas elecciones están justificadas porque los grafos son la estructura natural para representar dependencias entre operaciones, permitiendo identificar qué operaciones pueden ejecutarse en paralelo y cuáles deben esperar resultados previos. Las tablas hash proporcionan búsqueda O(1) para variables, y los arrays dinámicos permiten crecimiento eficiente del grafo. Los espacios de iteración como hiper-rectángulos permiten análisis matemático formal de dependencias y optimización de loops.

## 4. What is the formal time and space complexity of your implementation, how does it scale as input size grows, and what factors dominate performance?

La complejidad temporal dominante es O(n²) debido a la eliminación de subexpresiones comunes (CSE), donde n es el número de operaciones. La construcción del grafo es O(n) amortizado, la fusión de operaciones es O(n), y la generación de código CUDA es O(n). La complejidad espacial es O(n + e) donde e es el número de aristas, que en el peor caso puede ser O(n²) si cada operación depende de todas las anteriores. A medida que crece el tamaño de entrada, el factor dominante es la búsqueda de duplicados en CSE, que escala cuadráticamente. Sin embargo, esto puede mejorarse a O(n) usando tablas hash en lugar de búsqueda lineal. Para la mayoría de casos prácticos con pocas operaciones, el rendimiento es aceptable, pero para programas muy grandes con cientos de operaciones, la optimización de CSE se vuelve el cuello de botella.

## 5. What experimental results were obtained, how do they support or contradict the theoretical expectations, and how was previously received feedback incorporated into the final system and results?

Los resultados experimentales muestran mejoras significativas: reducción del 50% en número de operaciones (de 6 a 3 nodos), 61% de reducción en accesos a memoria, y 99% de reducción en operaciones computacionales redundantes. Estos resultados apoyan las expectativas teóricas de que las optimizaciones de fusión y eliminación de duplicados pueden reducir significativamente el costo computacional. La reducción de 99% en computación es incluso mejor de lo esperado, probablemente porque eliminamos cálculos duplicados que no eran obvios inicialmente. El feedback recibido durante el desarrollo, especialmente sobre problemas de parsing y formato de entrada, se incorporó mejorando el manejo de errores y documentando claramente el formato correcto de uso. También se identificaron limitaciones como la complejidad O(n²) de CSE y la falta de verificación completa de shapes, las cuales se documentaron para futuras mejoras.

## 6. What experimental results were obtained, how do they support or contradict the theoretical expectations, and how was previously received feedback incorporated into the final system and results?

Los resultados experimentales obtenidos demuestran que el compilador logra optimizaciones significativas: reducción del 50% en operaciones del grafo, 61% de reducción en accesos a memoria, y 99% de reducción en computación redundante. Estos resultados confirman que el enfoque de construir un grafo y aplicar optimizaciones funciona en la práctica. La reducción de 99% en computación supera las expectativas teóricas, indicando que hay más redundancia en el código original de la que anticipábamos. Durante el desarrollo, recibimos feedback sobre problemas de compatibilidad con diferentes shells y formato de entrada, lo cual se resolvió documentando claramente el uso correcto con printf y creando scripts de ejemplo. También se incorporó feedback sobre la necesidad de visualizaciones y análisis detallado, resultando en la creación de gráficas de optimización y documentación completa de algoritmos y limitaciones.

